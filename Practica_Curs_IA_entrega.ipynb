{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Practica Curs IA -  entrega.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32CurIM10Ef1"
      },
      "source": [
        "# Curs IA  Octubre 2021 - Pràctica\n",
        "## Part de definició del model per la paràctica de classificació de departament\n",
        "**Proves amb les dades classificades i resumides per tenir menys categories**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo9yr8vez6el"
      },
      "source": [
        "Els requeriments per executar aquest quadern són els següents:\n",
        "- Arxiu model.bin ubicat a la carpeta \"./34/\". Conté el model amb el vector de característiques de gairebé 800mil paraules en català. Aquest arxiu l'obtenim de la pàgina http://vectors.nlpl.eu/repository/, concretament de la URL http://vectors.nlpl.eu/repository/20/34.zip\n",
        "- Arxiu tweets.csv. Aquest arxiu conté els tweets obtinguts.\n",
        "\n",
        "\n",
        "L'arxiu tweets.csv haurà de contenir, com a mínim, les columnes:\n",
        "- text\n",
        "- classe\n",
        "\n",
        "\n",
        "La columna classe haurà d'estar codificada de 0 a N - 1, essent N el nombre total de classes. En el nostre cas departaments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3KJ1f_d49_R",
        "outputId": "faa8ea77-513f-4db2-8362-1bb3087211be"
      },
      "source": [
        "# Aquest codi permet descarregar el model en català del vector de característiques de cada paraula\n",
        "\n",
        "!wget http://vectors.nlpl.eu/repository/20/34.zip\n",
        "!unzip 34.zip\n",
        "!ls\n",
        "!rm 34.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 13:06:32--  http://vectors.nlpl.eu/repository/20/34.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 602902722 (575M) [application/zip]\n",
            "Saving to: ‘34.zip’\n",
            "\n",
            "34.zip              100%[===================>] 574.97M  25.7MB/s    in 23s     \n",
            "\n",
            "2021-11-25 13:06:56 (25.0 MB/s) - ‘34.zip’ saved [602902722/602902722]\n",
            "\n",
            "Archive:  34.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n",
            "34.zip\t\t    LIST       model.bin  README\n",
            "dadespracresum.tsv  meta.json  model.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFLNtqDb0KD6"
      },
      "source": [
        "## Referències"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbS2pdn0M9p"
      },
      "source": [
        "- Exmple de classificació de textos fent servir CNN: https://cezannec.github.io/CNN_Text_Classification/\n",
        "- Corpus d'idiomes amb els seus models d'embedding: http://vectors.nlpl.eu/repository/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BENlWJip0ZGH"
      },
      "source": [
        "## Inici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsI6lnM-0Qv9"
      },
      "source": [
        "Comencem important les llibreries necessàries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3-3wYST0dRN"
      },
      "source": [
        "import pandas as pd                                   # tractament d'arxius i dades\n",
        "import re                                             # mòdul d'expressions regulars\n",
        "import numpy as np                                    # llibreria multifuncional (visualització i tractament de daddes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # funció que ens permetrà dividir les dades en dades d'entrenament, de test i de validació\n",
        "\n",
        "from sklearn import preprocessing                     # la farem servir per passar els valors de la columna que conté les\n",
        "                                                      # etiquetes (en el nostre cas els departaments) a valors numèrics del\n",
        "                                                      # 0 al N-1 departaments.\n",
        "                                                      # si l'arxiu que conté les dades d'entrenament ja conté la columna\n",
        "                                                      # de les etiquetes codificades entre 0 i N-1 no caldrà fer-la servir\n",
        "\n",
        "from gensim.models import KeyedVectors                # funció que permet carregar un model d'embedding d'un idioma\n",
        "                                                      # aquest model conté el vector de característiques d'un diccionari\n",
        "                                                      # de paraules prèviament entrenat.\n",
        "                                                      # en el nostre cas fem servir un corpus en català amb quasi 800mil\n",
        "                                                      # paraules\n",
        "\n",
        "import torch                                          # llibreria d'aprenentatge automàtic\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCGikhXKE61V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsSu9zGu1OyD"
      },
      "source": [
        "## Tractament de les dades\n",
        "### Obtenció de les dades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_LgaTtx1Qwp"
      },
      "source": [
        "Llegim l'arxiu CSV amb els tweets i mostrem informació sobre les dades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad45koCc1RgE",
        "outputId": "66f39547-8554-4444-9bc9-cbf541c1fb0b"
      },
      "source": [
        "dades = pd.read_csv('dadespracresum.tsv', sep='\\t')\n",
        "dades.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(475, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BSys2n1O2Y_1",
        "outputId": "4fcbbc55-4c4d-4a61-b924-7c44f118e92a"
      },
      "source": [
        "dades.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>arxiu</th>\n",
              "      <th>hash</th>\n",
              "      <th>classet</th>\n",
              "      <th>classe</th>\n",
              "      <th>classer</th>\n",
              "      <th>text_orig</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bcn_cat_tweet_20211021_203425.csv</td>\n",
              "      <td>8322723a13e516aeb6b9a358f757de02ad8c105ae0aa0f...</td>\n",
              "      <td>5 Urbanisme</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>RT @aitooru: @bcn_ajuntament Villaroel amb Gra...</td>\n",
              "      <td>rt   villaroel amb gran viaabans aparcaments a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bcn_cat_tweet_20211021_203425.csv</td>\n",
              "      <td>dd4677846d72141cced6f34ae1f02a3403968c01eba79f...</td>\n",
              "      <td>5 Urbanisme</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>RT @Dr_diez: @bcn_ajuntament Deixar ela carrer...</td>\n",
              "      <td>rt   deixar ela carrers pagats de formigó comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bcn_cat_tweet_20211021_203425.csv</td>\n",
              "      <td>86c491271d5d456f27fe375b0cc0ce9323eb8085cd5d6c...</td>\n",
              "      <td>2 Politica</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>@bcn_ajuntament Heu destrossat Barcelona! Cost...</td>\n",
              "      <td>heu destrossat barcelona costarà anys reconstr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bcn_cat_tweet_20211021_203425.csv</td>\n",
              "      <td>61a17072a12a1d64813327df9a2b1a0e6f609b8e8b446a...</td>\n",
              "      <td>3 Serveis Municipals</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>RT @AntoniaGiro: @bcn_ajuntament Si treballes ...</td>\n",
              "      <td>rt   si treballes en precari i mal pagat en un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bcn_cat_tweet_20211021_203425.csv</td>\n",
              "      <td>a6943dc1c60ba6699cb6c2b9eed8931ddd10737a6e6c24...</td>\n",
              "      <td>8 OAC</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>@bcn_ajuntament , bona tarde, si us plau em pr...</td>\n",
              "      <td>bona tarde si us plau em pregunta ma germana q...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               arxiu  ...                                               text\n",
              "0  bcn_cat_tweet_20211021_203425.csv  ...  rt   villaroel amb gran viaabans aparcaments a...\n",
              "1  bcn_cat_tweet_20211021_203425.csv  ...  rt   deixar ela carrers pagats de formigó comp...\n",
              "2  bcn_cat_tweet_20211021_203425.csv  ...  heu destrossat barcelona costarà anys reconstr...\n",
              "3  bcn_cat_tweet_20211021_203425.csv  ...  rt   si treballes en precari i mal pagat en un...\n",
              "4  bcn_cat_tweet_20211021_203425.csv  ...  bona tarde si us plau em pregunta ma germana q...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IueS0VZ92jt9"
      },
      "source": [
        "Ens quedem amb les columnes que ens interessen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5m6JlF222kAZ",
        "outputId": "2c99722d-dfc5-41cb-ca75-5aac12f82fba"
      },
      "source": [
        "dades_utils = dades.drop(['arxiu', 'hash', 'text_orig','classet','classe'], axis=1)\n",
        "dades_utils.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classer</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>rt   villaroel amb gran viaabans aparcaments a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>rt   deixar ela carrers pagats de formigó comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>heu destrossat barcelona costarà anys reconstr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>rt   si treballes en precari i mal pagat en un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>bona tarde si us plau em pregunta ma germana q...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   classer                                               text\n",
              "0        2  rt   villaroel amb gran viaabans aparcaments a...\n",
              "1        2  rt   deixar ela carrers pagats de formigó comp...\n",
              "2        1  heu destrossat barcelona costarà anys reconstr...\n",
              "3        2  rt   si treballes en precari i mal pagat en un...\n",
              "4        3  bona tarde si us plau em pregunta ma germana q..."
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_pSNqdM3TVw"
      },
      "source": [
        "Definim una funció per netejar les dades. Traurem les emoticones, hashtags, referències a altres comptes de Tweeter, URLs, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tirvt2JO3beP"
      },
      "source": [
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "def neteja_text(x):\n",
        "    txt = x\n",
        "    # treu emojis\n",
        "    txt = emoji_pattern.sub(r' ', txt)\n",
        "    # treu les referències a altres comptes (@usuari)\n",
        "    txt = re.sub(\"@\\S+\", \"\", txt)\n",
        "    # treu els hashtags\n",
        "    txt = re.sub(\"#\\S+\", \"\", txt)\n",
        "    # treu el text \"RT\"\n",
        "    txt = re.sub(r'^rt[\\s]+', '', txt)\n",
        "    # treu els hiperenllaços\n",
        "    txt = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)', '', txt)\n",
        "    txt = re.sub(r\"[!#$%&'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]\", ' ', txt)\n",
        "    # minúscules\n",
        "    txt = txt.lower()\n",
        "    return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9_hfmrhy37gp",
        "outputId": "70c11d15-edd1-4905-d955-cab34e1758f7"
      },
      "source": [
        "dades_utils['text_net'] = dades_utils['text'].apply(neteja_text)\n",
        "dades_utils.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classer</th>\n",
              "      <th>text</th>\n",
              "      <th>text_net</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>rt   villaroel amb gran viaabans aparcaments a...</td>\n",
              "      <td>villaroel amb gran viaabans aparcaments a la c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>rt   deixar ela carrers pagats de formigó comp...</td>\n",
              "      <td>deixar ela carrers pagats de formigó comprat a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>heu destrossat barcelona costarà anys reconstr...</td>\n",
              "      <td>heu destrossat barcelona costarà anys reconstr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>rt   si treballes en precari i mal pagat en un...</td>\n",
              "      <td>si treballes en precari i mal pagat en una emp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>bona tarde si us plau em pregunta ma germana q...</td>\n",
              "      <td>bona tarde si us plau em pregunta ma germana q...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   classer  ...                                           text_net\n",
              "0        2  ...  villaroel amb gran viaabans aparcaments a la c...\n",
              "1        2  ...  deixar ela carrers pagats de formigó comprat a...\n",
              "2        1  ...  heu destrossat barcelona costarà anys reconstr...\n",
              "3        2  ...  si treballes en precari i mal pagat en una emp...\n",
              "4        3  ...  bona tarde si us plau em pregunta ma germana q...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt1CyOYU5iQc"
      },
      "source": [
        "### Tokenització de les dades\n",
        "Carreguem el model amb el vector de característiques en català i convertim a tokens els textos llegits de l'arxiu d'entrada.\n",
        "Si no trobem la paraula al corpus li assignem un 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kOqsUUZ4QGu"
      },
      "source": [
        "model_embedding = KeyedVectors.load_word2vec_format('model.bin', binary = True)\n",
        "\n",
        "def tokenitza_textos(model_embedding, textos):\n",
        "    paraules = [tweet.split() for tweet in textos]\n",
        "\n",
        "    textos_tokenitzats = []\n",
        "    for tweet in paraules:\n",
        "        valors = []\n",
        "        for paraula in tweet:\n",
        "            try:\n",
        "                index = model_embedding.vocab[paraula].index\n",
        "            except: \n",
        "                index = 0\n",
        "            valors.append(index)\n",
        "        textos_tokenitzats.append(valors)\n",
        "    \n",
        "    return textos_tokenitzats\n",
        "\n",
        "tt = tokenitza_textos(model_embedding, dades_utils['text_net'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJg7bPnCLsip",
        "outputId": "3f6d27f3-bafb-49d5-bcc0-de55c725b74b"
      },
      "source": [
        " dades_utils['text_net']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      villaroel amb gran viaabans aparcaments a la c...\n",
              "1      deixar ela carrers pagats de formigó comprat a...\n",
              "2      heu destrossat barcelona costarà anys reconstr...\n",
              "3      si treballes en precari i mal pagat en una emp...\n",
              "4      bona tarde si us plau em pregunta ma germana q...\n",
              "                             ...                        \n",
              "470    tanmateix et demanem que esborris i no pengis ...\n",
              "471    per aquest motiu és que en cas dapagada aconse...\n",
              "472    actualment continuen les revisions dels equips...\n",
              "473    daquesta zona de lenllumenat públic i no és se...\n",
              "474                    lo de baixar el ibi per altre dia\n",
              "Name: text_net, Length: 475, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3lYz-7b79lw"
      },
      "source": [
        "### Normalització de la matriu d'entrada del model\n",
        "Tenint en compte que la CNN necessita una quantitat fixa de columnes d'entrada hem d'assegurar que la matriu que representa els textos i els tokens tingui un nombre de columnes determinat. Aquest procés es coneix com a padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyE05CQ-8rWt"
      },
      "source": [
        "La matriu resultant del padding conté la X del model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZKZCW2i7EyK"
      },
      "source": [
        "def padding(tt, longitud):\n",
        "    caract = np.zeros((len(tt), longitud), dtype=int)\n",
        "\n",
        "    for i, fila in enumerate(tt):\n",
        "        caract[i, -len(fila):] = np.array(fila)[:longitud]\n",
        "    \n",
        "    return caract\n",
        "\n",
        "# Definim una matriu amb 200 columnes\n",
        "X = padding(tt, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSET1TXA9a1t"
      },
      "source": [
        "### Preparar etiquetes\n",
        "Convertim el tipus de dades que conté les etiquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTlCbhQ9bAo"
      },
      "source": [
        "etiquetes = dades_utils['classer'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KezHJvw-OKi"
      },
      "source": [
        "## Model CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDKO6Yo6-YHA"
      },
      "source": [
        "### Preparació de les dades d'entrada del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnY9y-0EFezK"
      },
      "source": [
        "#### Divisió de les dades d'entrada en entrenament, test i validació"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGvl8Tv--UAR"
      },
      "source": [
        "train_x, X_resta, train_y, y_resta = train_test_split(X,\n",
        "                                                    etiquetes,\n",
        "                                                    test_size = 0.3,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "# dividim la resta de dades que no són entrenament en test i validació\n",
        "test_idx = int(len(X_resta) * 0.5)\n",
        "val_x, test_x = X_resta[:test_idx], X_resta[test_idx:]\n",
        "val_y, test_y = y_resta[:test_idx], y_resta[test_idx:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5CO-39iFlhM"
      },
      "source": [
        "#### Conversió de les dades d'entrada del model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLBZSET4FDRG"
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQmnckiF7A8"
      },
      "source": [
        "### Definició de l'arquitectura del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBNBXosoGwpB"
      },
      "source": [
        "Abans comprovem si està disponible el motor d'execució de la GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsj9bumYF9-1",
        "outputId": "47770119-bc2b-44c2-d1b9-6c8697cb9e81"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, training on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz5wLC08GJkc"
      },
      "source": [
        "\n",
        "class ClassificaCNN(nn.Module):\n",
        "    \"\"\"\n",
        "     model_embedding:              model que conté el corpus de l'idioma (aquest conté el vector de N caracterísques de cada paraula)\n",
        "     mida_vocabulari:              quantitat de paraules del corpus\n",
        "     mida_sortida:                 quantitat de resultats, en aquest cas és el número de classes\n",
        "     mida_vector_caracteristiques: longitud del vector de característiques del corpus\n",
        "     num_filtres:                  nombre de filtres que es faran servir a la convolució\n",
        "     mides_kernels:                mides dels kernels a aplicar ==> els kernels seran de:\n",
        "                                      [3, 100], [4, 100] i [5, 100]    ==> [3 o 4 o 5, mida_vector_caracteristiques]\n",
        "     freeze_embeddings:            documentació oficial \"If True, the tensor does not get updated in the learning process\"\n",
        "     drop_prob:                    probabilitat a aplicar a la capa de dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 model_embedding,\n",
        "                 mida_vocabulari,\n",
        "                 mida_sortida,\n",
        "                 mida_vector_caracteristiques,\n",
        "                 num_filtres = 100,\n",
        "                 mides_kernels = [3, 4, 5],\n",
        "                 freeze_embeddings = True,\n",
        "                 drop_prob = 0.5):\n",
        "\n",
        "        super(ClassificaCNN, self).__init__()\n",
        "\n",
        "        self.num_filtres = num_filtres\n",
        "        self.mida_vector_caracteristiques = mida_vector_caracteristiques\n",
        "        \n",
        "        # 1. capa d'embedding\n",
        "        self.embedding = nn.Embedding(mida_vocabulari, mida_vector_caracteristiques)\n",
        "        #    li passem els pesos del model_embedding a la capa\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(model_embedding.vectors))\n",
        "        #    (opcional) Documentació oficial \"If True, the tensor does not get updated in the learning process\"\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "        \n",
        "        # 2. capes convolucionals\n",
        "        #    Es creen tantes capes convolucionals com kernels vulguem, per defecte són 3.\n",
        "        #    L'entrada de cada capa és 1: 1 paraula\n",
        "        #    La sortida de cada cap és igual a la mida del vector de característiques del corpus, normalment 100, 200 o 300\n",
        "        self.convs_1d = nn.ModuleList([nn.Conv2d(1, \n",
        "                                                 num_filtres, \n",
        "                                                 (k, mida_vector_caracteristiques),    # [3, 100], [4, 100] i [5, 100]\n",
        "                                                 padding = (k - 2, 0))                 # (1, 0), (2, 0) i (3, 0)\n",
        "                                       for k in mides_kernels\n",
        "                                      ]\n",
        "                                     )\n",
        "        \n",
        "        # 3. capa fully-connected per la classificació final\n",
        "        self.fc = nn.Linear(len(mides_kernels) * num_filtres, mida_sortida) \n",
        "        \n",
        "        # 4. capa de dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "    \n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.relu(conv(x)).squeeze(3)\n",
        "        \n",
        "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x_max\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = embeds.unsqueeze(1)\n",
        "        \n",
        "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
        "        \n",
        "        x = torch.cat(conv_results, 1)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # fem un flatten de la matriu a vector\n",
        "        x = x.view(-1, len(mides_kernels) * num_filtres)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgMVjwdG0Ct"
      },
      "source": [
        "Definim els hiperparàmetres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdGdZ4xfGiA6",
        "outputId": "7eb6bfd6-9aa7-49df-b335-87b568c50970"
      },
      "source": [
        "mida_vocabulari = len(model_embedding.index2word)           # quantitat de paraules del corpus en català\n",
        "\n",
        "mida_sortida = 4                                          # mida del resultat. \n",
        "                                                            # ÉS MOLT IMPORTANT QUE LES DADES DE LES ETIQUETES y_train, y_test \n",
        "                                                            # tinguin els valors compressos entre 0 i (mida_sortida - 1) de cara\n",
        "                                                            # a calcular la funció de costos CrossEntropyLoss\n",
        "        \n",
        "mida_vector_caracteristiques = model_embedding.vector_size  # longitud del vector de característiques del corpus en català\n",
        "num_filtres = 100                                           # nombre de filtres que es faran servir a la convolució\n",
        "mides_kernels = [3, 4, 5]                                   # mides dels kernels a aplicar\n",
        "\n",
        "net = ClassificaCNN(model_embedding, mida_vocabulari, mida_sortida, mida_vector_caracteristiques, num_filtres, mides_kernels)\n",
        "                    \n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassificaCNN(\n",
            "  (embedding): Embedding(799020, 100)\n",
            "  (convs_1d): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))\n",
            "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1), padding=(2, 0))\n",
            "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1), padding=(3, 0))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=4, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLg3tT6AG6nr"
      },
      "source": [
        "## Entrenament del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ReFlXsQHEXb"
      },
      "source": [
        "Definim la ràtio d'entrenament, la funció de costos i l'optimitzador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G60oaVmyHASP"
      },
      "source": [
        "lr = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr = lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_TYiHwHheB"
      },
      "source": [
        "def train(net, train_loader, epochs):\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        net.train()\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            if train_on_gpu:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            inputs = inputs.type(torch.LongTensor)\n",
        "            output = net(inputs)\n",
        "            loss = criterion(output.squeeze(), labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        net.eval()\n",
        "        for batch_idx, (inputs, labels) in enumerate(valid_loader):\n",
        "            if train_on_gpu:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            inputs = inputs.type(torch.LongTensor)\n",
        "            output = net(inputs)\n",
        "            loss = criterion(output.squeeze(), labels.long())\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        train_loss = train_loss/len(train_loader.sampler)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "        print('Època: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
        "\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss disminueix ({:.6f} --> {:.6f}).  Desant el model ...'.format(valid_loss_min, valid_loss))\n",
        "            torch.save(net.state_dict(), 'model_classificacio.pt')\n",
        "            valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zPWw9L__Gok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56001d64-f97b-43b2-8328-6931f8647022"
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "train(net, train_loader, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Època: 1 \tTraining Loss: 1.216590 \tValidation Loss: 1.376654\n",
            "Validation loss disminueix (inf --> 1.376654).  Desant el model ...\n",
            "Època: 2 \tTraining Loss: 1.042200 \tValidation Loss: 1.344534\n",
            "Validation loss disminueix (1.376654 --> 1.344534).  Desant el model ...\n",
            "Època: 3 \tTraining Loss: 0.975907 \tValidation Loss: 1.246694\n",
            "Validation loss disminueix (1.344534 --> 1.246694).  Desant el model ...\n",
            "Època: 4 \tTraining Loss: 0.901334 \tValidation Loss: 1.233034\n",
            "Validation loss disminueix (1.246694 --> 1.233034).  Desant el model ...\n",
            "Època: 5 \tTraining Loss: 0.840562 \tValidation Loss: 1.187497\n",
            "Validation loss disminueix (1.233034 --> 1.187497).  Desant el model ...\n",
            "Època: 6 \tTraining Loss: 0.760701 \tValidation Loss: 1.180666\n",
            "Validation loss disminueix (1.187497 --> 1.180666).  Desant el model ...\n",
            "Època: 7 \tTraining Loss: 0.726488 \tValidation Loss: 1.162419\n",
            "Validation loss disminueix (1.180666 --> 1.162419).  Desant el model ...\n",
            "Època: 8 \tTraining Loss: 0.632538 \tValidation Loss: 1.133506\n",
            "Validation loss disminueix (1.162419 --> 1.133506).  Desant el model ...\n",
            "Època: 9 \tTraining Loss: 0.571532 \tValidation Loss: 1.114460\n",
            "Validation loss disminueix (1.133506 --> 1.114460).  Desant el model ...\n",
            "Època: 10 \tTraining Loss: 0.505193 \tValidation Loss: 1.118215\n",
            "Època: 11 \tTraining Loss: 0.448614 \tValidation Loss: 1.095236\n",
            "Validation loss disminueix (1.114460 --> 1.095236).  Desant el model ...\n",
            "Època: 12 \tTraining Loss: 0.377700 \tValidation Loss: 1.134400\n",
            "Època: 13 \tTraining Loss: 0.316236 \tValidation Loss: 1.124888\n",
            "Època: 14 \tTraining Loss: 0.277942 \tValidation Loss: 1.131584\n",
            "Època: 15 \tTraining Loss: 0.226441 \tValidation Loss: 1.122702\n",
            "Època: 16 \tTraining Loss: 0.203032 \tValidation Loss: 1.120606\n",
            "Època: 17 \tTraining Loss: 0.168569 \tValidation Loss: 1.215067\n",
            "Època: 18 \tTraining Loss: 0.140298 \tValidation Loss: 1.212829\n",
            "Època: 19 \tTraining Loss: 0.120379 \tValidation Loss: 1.165386\n",
            "Època: 20 \tTraining Loss: 0.099176 \tValidation Loss: 1.212145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G48t3KTtJYPU"
      },
      "source": [
        "## Test del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTn5Lp1Q_d5G"
      },
      "source": [
        "Carreguem el model desat a la fase d'entrenament"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPFa7r88_c_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64da7e3-b0d8-44e7-fd88-d97758adea0f"
      },
      "source": [
        "net.load_state_dict(torch.load('model_classificacio.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7uBBduB_ot-"
      },
      "source": [
        "Definim el vector que conté les nostres classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QZhQ6d5_nrm"
      },
      "source": [
        "classes = [\"Altres\",\"Politica i hisenda\",\"Ciutat\",\"Ciutada i Seguretat\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPukCs3JbAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39df7fde-f81c-47eb-b5fc-d232cbfe9435"
      },
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(mida_sortida))\n",
        "class_total = list(0. for i in range(mida_sortida))\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
        "    if train_on_gpu:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    output = net(inputs)\n",
        "    loss = criterion(output.squeeze(), labels.long())\n",
        "    test_loss += loss.item() * inputs.size(0)\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    for i in range(len(inputs)):\n",
        "        label = labels.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(mida_sortida):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.900101\n",
            "\n",
            "Test Accuracy of Altres: 33% ( 1/ 3)\n",
            "Test Accuracy of Politica i hisenda: 41% ( 7/17)\n",
            "Test Accuracy of Ciutat: 93% (40/43)\n",
            "Test Accuracy of Ciutada i Seguretat:  0% ( 0/ 9)\n",
            "\n",
            "Test Accuracy (Overall): 66% (48/72)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26hqnbztIWVu"
      },
      "source": [
        "## Part de desplegament del model amb Gradio.app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFWTLMkFIdh8"
      },
      "source": [
        "Instal·lem el Gradio i carreguem la llibreria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWryRWXRIfHm",
        "outputId": "ec82e725-793b-4df9-bc8f-ca2b61288021"
      },
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-2.4.6-py3-none-any.whl (979 kB)\n",
            "\u001b[K     |████████████████████████████████| 979 kB 27.4 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.11.0-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 37.8 MB/s \n",
            "\u001b[?25hCollecting flask-cachebuster\n",
            "  Downloading Flask-CacheBuster-1.0.0.tar.gz (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.1.5)\n",
            "Collecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting markdown2\n",
            "  Downloading markdown2-2.4.1-py2.py3-none-any.whl (34 kB)\n",
            "Collecting Flask-Login\n",
            "  Downloading Flask_Login-0.5.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.8.0-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.19.5)\n",
            "Collecting Flask-Cors>=3.0.8\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Flask>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from gradio) (1.1.4)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (7.1.2)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from Flask-Cors>=3.0.8->gradio) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.1.1->gradio) (2.0.1)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2018.9)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 31.8 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 34.7 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n",
            "Building wheels for collected packages: ffmpy, flask-cachebuster\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4710 sha256=a39dfc608b93875bace8e955bc0d955f8d26e7e3af52716c79151be84cf9defc\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flask-cachebuster: filename=Flask_CacheBuster-1.0.0-py3-none-any.whl size=3371 sha256=9eee643eb7efe119088303ef42e625b6671d4838280cb5a99a4c66f8fdb59198\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/c0/c4/44687421dab41455be93112bd1b0dee1f3c5a9aa27bee63708\n",
            "Successfully built ffmpy flask-cachebuster\n",
            "Installing collected packages: pynacl, monotonic, cryptography, bcrypt, backoff, pydub, pycryptodome, paramiko, markdown2, Flask-Login, Flask-Cors, flask-cachebuster, ffmpy, analytics-python, gradio\n",
            "Successfully installed Flask-Cors-3.0.10 Flask-Login-0.5.0 analytics-python-1.4.0 backoff-1.10.0 bcrypt-3.2.0 cryptography-36.0.0 ffmpy-0.3.0 flask-cachebuster-1.0.0 gradio-2.4.6 markdown2-2.4.1 monotonic-1.6 paramiko-2.8.0 pycryptodome-3.11.0 pydub-0.25.1 pynacl-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bllX9VBwIiAp"
      },
      "source": [
        "Despleguem l'app a gradio\n",
        "\n",
        "Repetim alguns dels passos anteirors aquí com a recordatori de que s'han de fer en un desplegament separat de l'entrenament"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9xGvU45ImU7"
      },
      "source": [
        "classes = [\"Altres\",\"Politica i hisenda\",\"Ciutat\",\"Ciutada i Seguretat\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiAbRB94IqGT"
      },
      "source": [
        "net.load_state_dict(torch.load('model_classificacio.pt'))\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "def neteja_text(x):\n",
        "    txt = x\n",
        "    # treu emojis\n",
        "    txt = emoji_pattern.sub(r' ', txt)\n",
        "    # treu les referències a altres comptes (@usuari)\n",
        "    txt = re.sub(\"@\\S+\", \"\", txt)\n",
        "    # treu els hashtags\n",
        "    txt = re.sub(\"#\\S+\", \"\", txt)\n",
        "    # treu el text \"RT\"\n",
        "    txt = re.sub(r'^rt[\\s]+', '', txt)\n",
        "    # treu els hiperenllaços\n",
        "    txt = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)', '', txt)\n",
        "    txt = re.sub(r\"[!#$%&'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]\", ' ', txt)\n",
        "    # minúscules\n",
        "    txt = txt.lower()\n",
        "    return txt\n",
        "\n",
        "model_embedding = KeyedVectors.load_word2vec_format('model.bin', binary = True)\n",
        "\n",
        "def tokenitza_textos(model_embedding, textos):\n",
        "    paraules = [tweet.split() for tweet in textos]\n",
        "\n",
        "    textos_tokenitzats = []\n",
        "    for tweet in paraules:\n",
        "        valors = []\n",
        "        for paraula in tweet:\n",
        "            try:\n",
        "                index = model_embedding.vocab[paraula].index\n",
        "            except: \n",
        "                index = 0\n",
        "            valors.append(index)\n",
        "        textos_tokenitzats.append(valors)\n",
        "    \n",
        "    return textos_tokenitzats\n",
        "\n",
        "def padding(tt, longitud):\n",
        "    caract = np.zeros((len(tt), longitud), dtype=int)\n",
        "\n",
        "    for i, fila in enumerate(tt):\n",
        "        caract[i, -len(fila):] = np.array(fila)[:longitud]\n",
        "    \n",
        "    return caract\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaxV9AC5I8FA"
      },
      "source": [
        "Definim la funció que farà la classificació i que passarem al gradio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCyGZVf2I9KE"
      },
      "source": [
        "def classifica_tweets(el_tweet):\n",
        "    tweet = []\n",
        "    tweet = [neteja_text(el_tweet)]\n",
        "    #print('el tweet: ', tweet)\n",
        "    tt = tokenitza_textos(model_embedding, tweet)\n",
        "    #print('tt: ', tt)\n",
        "    X = padding(tt, 200)\n",
        "    #print('X: ', X)\n",
        "\n",
        "    net.eval()\n",
        "    X = torch.from_numpy(X).type(torch.LongTensor)\n",
        "    if train_on_gpu:\n",
        "        net().cuda()\n",
        "        X = X.cuda()\n",
        "\n",
        "    output = F.softmax(net(X), dim=1)\n",
        "    #print('output: ', output)\n",
        "\n",
        "    return {classes[i]: float(output[0][i]) for i in range(len(classes))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktVTReDQJBTW"
      },
      "source": [
        "Despleguem el model a gradio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "GNXa4DfRJHLE",
        "outputId": "cfe96874-afad-4584-9acf-190877a6191e"
      },
      "source": [
        "inputs = gr.inputs.Textbox(lines=5, label=\"Enganxa el Tweet aquí: \")\n",
        "outputs = gr.outputs.Label(num_top_classes=3)\n",
        "gr.Interface(fn=classifica_tweets, inputs=inputs, outputs=outputs).launch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://51023.gradio.app\n",
            "\n",
            "This share link will expire in 72 hours. To get longer links, send an email to: support@gradio.app\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://51023.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f481d290b50>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7860/',\n",
              " 'https://51023.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jJuAnB_OdE3"
      },
      "source": [
        "**Fins aquí l'execució de la pàctica**\n",
        "\n",
        "# Notebook utilitzat per l'obtenció dels tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB8GLCIZlYTA"
      },
      "source": [
        "# coding:utf-8\n",
        "\n",
        "import tweepy\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime, date, timedelta\n",
        "import re\n",
        "\n",
        "#Obtenim data actual\n",
        "today = datetime.today()\n",
        "#Especifiquem un rang de dates per obtenir tweets (Exemple 20 dies)\n",
        "tweet_begin_date = datetime.strftime(today - timedelta(days=20), '%Y-%m-%d_00:00:00')\n",
        "tweet_end_date = datetime.strftime((today), '%Y-%m-%d_23:59:00')\n",
        "\n",
        "\n",
        "# Guardem CSV a arrel del projecte\n",
        "csv_dir = './'\n",
        "\n",
        "# Twitter API KEY - Obtingudes servei API Twitter, user @regonzalezmas\n",
        "Consumer_key = \"MGVRn15j2DLuDXmimfKV0s2vs\"\n",
        "Consumer_secret = \"Ah8a6HkFvgsWkwAS3MY7uftq8Rh6TIPUQKOwxw8H9rjVCikxtk\"\n",
        "Access_token =\"240674330-l6x04wZ59zJni7Aoy37H5re9raCDYPkJrZH3wuFS\"\n",
        "Access_secret = \"zyJsBlM4SgpflawPFOYd9bUZNq0zyxaArLN2dWjIPhGvR\"\n",
        "\n",
        "\n",
        "#Autenticació Twitter API \n",
        "def authTwitter():\n",
        "        auth = tweepy.OAuthHandler(Consumer_key, Consumer_secret)\n",
        "        auth.set_access_token(Access_token, Access_secret)\n",
        "        api = tweepy.API(auth, retry_count=3,retry_delay=40,retry_errors=set([401, 404, 500, 502, 503, 504]), wait_on_rate_limit = True, wait_on_rate_limit_notify=True)\n",
        "        return(api)\n",
        "\n",
        "#Funció per obtenir tweets\n",
        "def get_tweet(s):\n",
        "        api = authTwitter() #Auth\n",
        "        tweet_list = []\n",
        "        tweet_id_list = []\n",
        "        user_id_list = []\n",
        "\n",
        "        tweets = tweepy.Cursor(api.search, q = s,     #String cerca\n",
        "                 include_entities = True,   \n",
        "                 tweet_mode = 'extended',   \n",
        "                 since = tweet_begin_date,    \n",
        "                 lang = 'ca').items()       #Idioma català\n",
        "\n",
        "        #Guardem tweet a una llista\n",
        "        for tweet in tweets:\n",
        "                tweet_list.append([tweet.id, tweet.user.screen_name, tweet.created_at, tweet.full_text.replace('\\n',''), tweet.favorite_count, tweet.retweet_count])\n",
        "                tweet_id_list.append(tweet.id)\n",
        "                user_id_list.append(tweet.user.screen_name)\n",
        "        #Sortida en fitxer csv\n",
        "        with open(csv_dir+'tweet_'+ today.strftime('%Y%m%d_%H%M%S') + '.csv', 'w',newline='',encoding='utf-8') as f:\n",
        "                writer = csv.writer(f, lineterminator='\\n')\n",
        "                writer.writerow([\"id\",\"user\",\"created_at\",\"text\",\"fav\",\"RT\"])\n",
        "                writer.writerows(tweet_list)\n",
        "        pass\n",
        "\n",
        "def main():\n",
        "        #get_tweet(\"(to:vallsajuntament OR to:reus_cat OR to:elvendrell_cat) lang:ca\")\n",
        "        get_tweet(\"(to:bcn_ajuntament) lang:ca\")\n",
        "        #get_tweet(\"(to:barcelona_010) lang:ca\")\n",
        "        #get_tweet(\"(to:TGNAjuntament) lang:ca\")\n",
        "        #get_tweet(\"(to:paerialleida) lang:ca\")\n",
        "        #get_tweet(\"(to:girona_cat) lang:ca\")\n",
        "        #get_tweet(\"(to:012) lang:ca\")\n",
        "        #get_tweet(\"(to:gencat) lang:ca\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "        main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QQosuTEO1BC"
      },
      "source": [
        "# Notebook utilitzat per el filtratge i generació de l'arxiu de dades de prova"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBU2CtFg6WUg"
      },
      "source": [
        "# Curs IA - Paràctica filtratge de dades\n",
        "Part de neteja de dades per la paràctica de classificació de departament\n",
        "### Generació dels CSV \n",
        "Generarà un arxiu csv amb el filtrat del tots els tweets sota diferents criteris, columnes resultat:\n",
        "* arxiu: arxiu origen\n",
        "* hash: Per identificar el tweet en cas de tenir feina a mitges i no tornar a classificar lo fet\n",
        "* classe: aquí caldrà posar manualment la classe (del desplegable donat)\n",
        "* text_orig:Text original del tweet (origen del hash)\n",
        "* text: text complet ja netejat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8tnjH1z6aZv"
      },
      "source": [
        "## Importacio inicial\n",
        "Cal importar a la carpeta **d1** les dades de tots els csv que volguem importar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYNfkgU56xIu",
        "outputId": "4b13da14-dfea-4488-d393-6ef3d6ab2853"
      },
      "source": [
        "!ls d1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bcn_cat_tweet_20211021_203425.csv  paerialleida_20211021_204148.csv\n",
            "girona_tweet_20211021_204406.csv   TGNAjuntament_tweet_20211021_203819.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyiGgDRo7srW"
      },
      "source": [
        "Ara creem la classe que ens gestionarà la importació, neteja i tokenització dels CSV i ens generarà un sol arxiu csv amb les dades com les volem\n",
        "\n",
        "Particularitat:\n",
        "* Millorat el filtre de emojis\n",
        "* Afegit usuarisprohibits amb els usuaris de que no volem agafar els tweets ja que son origen  i no respostes\n",
        "* Trets els noms que comencen amb @ per ser referencies no útils\n",
        "* Evito duplicats\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUSQQjbK6Xqe",
        "outputId": "2edffadd-75e3-42dc-b34e-0a57d4db0d76"
      },
      "source": [
        "import csv\n",
        "import nltk  \n",
        "from nltk import tokenize\n",
        "import string\n",
        "import re\n",
        "import hashlib\n",
        "from os import listdir\n",
        "from os.path import join,isfile\n",
        "\n",
        "#http://latel.upf.edu/morgana/altres/pub/ca_stop.htm\n",
        "sepcatala=\"',a,abans,abans-d'ahir,abintestat,ací,adesiara,adés,adéu,adàgio,ah,ahir,ai,aitambé,aitampoc,aitan,aitant,aitantost,aixà,això,així,aleshores,algun,alguna,algunes,alguns,algú,alhora,allà,allèn,allò,allí,almenys,alto,altra,altre,altres,altresí,altri,alça,al·legro,amargament,amb,ambdues,ambdós,amunt,amén,anc,andante,andantino,anit,ans,antany,apa,aprés,aqueix,aqueixa,aqueixes,aqueixos,aqueixs,aquell,aquella,aquelles,aquells,aquest,aquesta,aquestes,aquests,aquèn,aquí,ara,arran,arrera,arrere,arreu,arri,arruix,atxim,au,avall,avant,aviat,avui,açò,bah,baix,baldament,ballmanetes,banzim-banzam,bastant,bastants,ben,bis,bitllo-bitllo,bo,bé,ca,cada,cal,cap,car,caram,catorze,cent,centes,cents,cerca,cert,certa,certes,certs,cinc,cinquanta,cinquena,cinquenes,cinquens,cinquè,com,comsevulla,contra,cordons,corrents,cric-crac,d,daixonses,daixò,dallonses,dallò,dalt,daltabaix,damunt,darrera,darrere,davall,davant,de,debades,dedins,defora,dejorn,dejús,dellà,dementre,dempeus,demés,demà,des,desena,desenes,desens,després,dessobre,dessota,dessús,desè,deu,devers,devora,deçà,diferents,dinou,dins,dintre,disset,divers,diversa,diverses,diversos,divuit,doncs,dos,dotze,dues,durant,ecs,eh,el,ela,elis,ell,ella,elles,ells,els,em,emperò,en,enans,enant,encara,encontinent,endalt,endarrera,endarrere,endavant,endebades,endemig,endemés,endemà,endins,endintre,enfora,engir,enguany,enguanyasses,enjús,enlaire,enlloc,enllà,enrera,enrere,ens,ensems,ensota,ensús,entorn,entre,entremig,entretant,entrò,envers,envides,environs,enviró,ençà,ep,ep,era,eren,eres,ergo,es,escar,essent,esser,est,esta,estada,estades,estan,estant,estar,estaran,estarem,estareu,estaria,estarien,estaries,estaré,estarà,estaràs,estaríem,estaríeu,estat,estats,estava,estaven,estaves,estem,estes,esteu,estic,estiguem,estigueren,estigueres,estigues,estiguessis,estigueu,estigui,estiguin,estiguis,estigué,estiguérem,estiguéreu,estigués,estiguí,estos,està,estàs,estàvem,estàveu,et,etc,etcètera,ets,excepte,fins,fora,foren,fores,força,fos,fossin,fossis,fou,fra,fui,fóra,fórem,fóreu,fóreu,fóssim,fóssiu,gaire,gairebé,gaires,gens,girientorn,gratis,ha,hagi,hagin,hagis,haguda,hagudes,hagueren,hagueres,haguessin,haguessis,hagut,haguts,hagué,haguérem,haguéreu,hagués,haguéssim,haguéssiu,haguí,hala,han,has,hauran,haurem,haureu,hauria,haurien,hauries,hauré,haurà,hauràs,hauríem,hauríeu,havem,havent,haver,haveu,havia,havien,havies,havíem,havíeu,he,hem,heu,hi,ho,hom,hui,hàgim,hàgiu,i,igual,iguals,inclusive,ja,jamai,jo,l,la,leri-leri,les,li,lla,llavors,llevat,lluny,llur,llurs,lo,los,ls,m,ma,mai,mal,malament,malgrat,manco,mant,manta,mantes,mantinent,mants,massa,mateix,mateixa,mateixes,mateixos,me,mentre,mentrestant,menys,mes,meu,meua,meues,meus,meva,meves,mi,mig,mil,mitges,mitja,mitjançant,mitjos,moixoni,molt,molta,moltes,molts,mon,mos,més,n,na,ne,ni,ningú,no,nogensmenys,només,noranta,nos,nosaltres,nostra,nostre,nostres,nou,novena,novenes,novens,novè,ns,nòs,nós,o,oh,oi,oidà,on,onsevulga,onsevulla,onze,pas,pengim-penjam,per,perquè,pertot,però,piano,pla,poc,poca,pocs,poques,potser,prest,primer,primera,primeres,primers,pro,prompte,prop,prou,puix,pus,pàssim,qual,quals,qualsevol,qualsevulla,qualssevol,qualssevulla,quan,quant,quanta,quantes,quants,quaranta,quart,quarta,quartes,quarts,quasi,quatre,que,quelcom,qui,quin,quina,quines,quins,quinze,quisvulla,què,ran,re,rebé,renoi,rera,rere,res,retruc,s,sa,salvament,salvant,salvat,se,segon,segona,segones,segons,seguida,seixanta,sempre,sengles,sens,sense,ser,seran,serem,sereu,seria,serien,series,seré,serà,seràs,seríem,seríeu,ses,set,setanta,setena,setenes,setens,setze,setè,seu,seua,seues,seus,seva,seves,si,sia,siau,sic,siguem,sigues,sigueu,sigui,siguin,siguis,sinó,sis,sisena,sisenes,sisens,sisè,sobre,sobretot,sol,sola,solament,soles,sols,som,son,sos,sota,sots,sou,sovint,suara,sí,sóc,són,t,ta,tal,tals,també,tampoc,tan,tanmateix,tant,tanta,tantes,tantost,tants,te,tercer,tercera,terceres,tercers,tes,teu,teua,teues,teus,teva,teves,ton,tos,tost,tostemps,tot,tota,total,totes,tothom,tothora,tots,trenta,tres,tret,tretze,tu,tururut,u,uf,ui,uix,ultra,un,una,unes,uns,up,upa,us,va,vagi,vagin,vagis,vaig,vair,vam,van,vares,vas,vau,vem,verbigràcia,vers,vet,veu,vint,vora,vos,vosaltres,vostra,vostre,vostres,vostè,vostès,vuit,vuitanta,vuitena,vuitenes,vuitens,vuitè,vés,vàreig,vàrem,vàreu,vós,xano-xano,xau-xau,xec,érem,éreu,és,ésser,àdhuc,àlies,ça,ço,òlim,ídem,últim,última,últimes,últims,únic,única,únics,úniques\".split(\",\")\n",
        "#usuaris  que volem treure del twiter\n",
        "usuarisprohibits=\"bcn_ajuntament,girona_cat,paerialleida,TGNAjuntament\".lower().split(\",\")\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", flags=re.UNICODE)\n",
        "referencies=re.compile(\"@\\S+\")                           \n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "class GenCSV:\n",
        "    stpwords=[]\n",
        "    usuarispr=[]\n",
        "    fets=[]\n",
        "    def __init__(self,stopwords=sepcatala,usuarispr=usuarisprohibits) -> None:\n",
        "        self.stpwords=stopwords \n",
        "        self.usproh=usuarispr        \n",
        "    def IngestaCSV(self, carpeta,nf,dialecte={\n",
        "            \"delimiter\":',',\n",
        "            \"doublequote\":True,\n",
        "            \"quotechar\":'\"'\n",
        "        }):\n",
        "        rs=[]\n",
        "        with open(join(carpeta,nf),encoding=\"utf-8\") as f:\n",
        "            for r in csv.DictReader(f,dialect=dialecte):\n",
        "                #no els tweets propis\n",
        "                if r[\"user\"] in self.usproh:\n",
        "                    continue\n",
        "                tx=r[\"text\"]                \n",
        "                hsh=hashlib.sha256(tx.encode(\"utf-8\")).hexdigest()\n",
        "                #no frases repetides\n",
        "                if hsh in self.fets:\n",
        "                    continue\n",
        "                self.fets.append(hsh)\n",
        "                rs.append([nf,hsh,0,tx,self.netejaText(tx)])\n",
        "        return rs\n",
        "    def netejaText(self,text):\n",
        "        #minucules\n",
        "        txt=text.lower()\n",
        "        #treiem emojis\n",
        "        txt=emoji_pattern.sub(r' ', txt)\n",
        "        #treiem arrobas\n",
        "        txt=referencies.sub('',txt)        \n",
        "        #puntuacio\n",
        "        txt= txt.translate(str.maketrans('', '', string.punctuation)) # el maketrans ens fa una taula de traduccio de 1er joc de caracters al 2n (aqui en blanc) i el 3er parametre topts els caracters que eliminara        \n",
        "        #els stopwords no els utilitzem ja que els tweets tenen molt poques paraules\n",
        "        return txt\n",
        "    def ProcessaCarpeta(self, carpeta,sortida):\n",
        "        larx=[f for f in listdir(carpeta) if isfile(join(carpeta, f))]\n",
        "        lns=[]\n",
        "        for nf in larx:\n",
        "            part=self.IngestaCSV(carpeta,nf)\n",
        "            lns.extend(part)\n",
        "        with open(sortida,'w',encoding=\"utf-8\",newline='') as fsortida:\n",
        "            writ=csv.writer(fsortida,delimiter='\\t',lineterminator='\\r\\n')\n",
        "            writ.writerow(['arxiu','hash','classe','text_orig','text'])\n",
        "            for ar in lns:\n",
        "                writ.writerow(ar)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsgT5X-W8HYA"
      },
      "source": [
        "Ara generem la sortida\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ4-6xO08JY4",
        "outputId": "7bd9fe54-4f0c-4b87-fe36-d1a619af35bf"
      },
      "source": [
        "g=GenCSV()\n",
        "print(g.ProcessaCarpeta(\"./d1\",\"./sort.csv\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    }
  ]
}